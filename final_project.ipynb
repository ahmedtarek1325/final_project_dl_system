{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Header "
      ],
      "metadata": {
        "id": "Be8Ydtqs6uQs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrTX3S_U56dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9049b915-0287-417e-beaa-d4c5c87a5870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/10714\n",
            "/content/drive/MyDrive/10714/final_project\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/dlsys10714/mugrade.git\n",
            "  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-ci884luf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-ci884luf\n",
            "  Resolved https://github.com/dlsys10714/mugrade.git to commit 98609ee80ee24bf278455b48aa8d06bd3f5d0430\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.8/dist-packages (2.10.3)\n"
          ]
        }
      ],
      "source": [
        "# Code to set up the assignment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "#!mkdir -p 10714\n",
        "%cd /content/drive/MyDrive/10714\n",
        "#!git clone https://github.com/dlsys10714/hw4.git\n",
        "%cd /content/drive/MyDrive/10714/final_project/\n",
        "\n",
        "!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
        "!pip3 install pybind11"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcAOD_Fq58ZU",
        "outputId": "b2b0291a-3492-4b63-db44-627fc1be13c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- Found pybind11: /usr/local/lib/python3.8/dist-packages/pybind11/include (found version \"2.10.3\")\n",
            "-- Found cuda, building cuda backend\n",
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/drive/MyDrive/10714/final_project/build\n",
            "make[1]: Entering directory '/content/drive/MyDrive/10714/final_project/build'\n",
            "make[2]: Entering directory '/content/drive/MyDrive/10714/final_project/build'\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/final_project/build'\n",
            "\u001b[35m\u001b[1mConsolidate compiler generated dependencies of target ndarray_backend_cpu\u001b[0m\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/final_project/build'\n",
            "[  0%] Built target ndarray_backend_cpu\n",
            "make[3]: Entering directory '/content/drive/MyDrive/10714/final_project/build'\n",
            "make[3]: Leaving directory '/content/drive/MyDrive/10714/final_project/build'\n",
            "[ 50%] Built target ndarray_backend_cuda\n",
            "make[2]: Leaving directory '/content/drive/MyDrive/10714/final_project/build'\n",
            "make[1]: Leaving directory '/content/drive/MyDrive/10714/final_project/build'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('./python')"
      ],
      "metadata": {
        "id": "9gUYF0vo5-om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extending Our Ops file \n",
        "In this section, we'll add extra functions to be able to build the transformer moddule. \n",
        "\n",
        "- bmm"
      ],
      "metadata": {
        "id": "KE9A8Ngq9fpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BMM"
      ],
      "metadata": {
        "id": "dMTUO3Gs9xPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -l -v -k \"bmm_forward\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B62bdkU495jN",
        "outputId": "bb48ed45-dd2a-46e5-8f2e-2832eff219d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.8.16, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/final_project, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 336 items / 333 deselected                                           \u001b[0m\n",
            "\n",
            "tests/test_ext_ops.py::test_bmm_forward[params0-device0] \u001b[32mPASSED\u001b[0m\u001b[36m          [ 33%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_bmm_forward[params1-device0] \u001b[32mPASSED\u001b[0m\u001b[36m          [ 66%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_bmm_forward[params2-device0] \u001b[32mPASSED\u001b[0m\u001b[36m          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=================== 3 passed, 333 deselected in 2.00 seconds ===================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m pytest -l -v -k \"bmm_backward\""
      ],
      "metadata": {
        "id": "iivcHRgQ95ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split and Concatenate"
      ],
      "metadata": {
        "id": "Y9qmGhQaYT1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -l -v -k \"concatenate_forward\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti5rnnvnYTlZ",
        "outputId": "6bdace0a-a6b5-4262-d6c2-c6c10820bf44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.8.16, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/final_project, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 336 items / 332 deselected                                           \u001b[0m\n",
            "\n",
            "tests/test_ext_ops.py::test_concatenate_forward[params0-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 25%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_concatenate_forward[params1-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 50%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_concatenate_forward[params2-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 75%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_concatenate_forward[params3-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=================== 4 passed, 332 deselected in 2.09 seconds ===================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -l -v -k \"concatenate_backward\""
      ],
      "metadata": {
        "id": "vTwDyHN-95ds"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Group"
      ],
      "metadata": {
        "id": "lE9qxswXottQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -l -v -k \"split_group_forward\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksVkmuaeouBJ",
        "outputId": "2eb4ff63-fcaf-4cdc-8706-3052518143b1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.8.16, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/final_project, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 343 items / 339 deselected                                           \u001b[0m\n",
            "\n",
            "tests/test_ext_ops.py::test_split_group_forward[params0-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 25%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_split_group_forward[params1-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 50%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_split_group_forward[params2-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [ 75%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_split_group_forward[params3-device0] \u001b[32mPASSED\u001b[0m\u001b[36m  [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m=================== 4 passed, 339 deselected in 2.01 seconds ===================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J5NLHyOqotcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4dZJ_lFotaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers Implementation \n"
      ],
      "metadata": {
        "id": "kH8p-Kih6VT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Head"
      ],
      "metadata": {
        "id": "7_5Wd9hp6rQv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7Wym8sF94lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-CLmWg5q94jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LSE51vp94hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KIHFk8yM94eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Numpy and Torch "
      ],
      "metadata": {
        "id": "1agbzKfs7DR0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import torch.nn as nn \n",
        "import torch "
      ],
      "metadata": {
        "id": "R9aNBmci6UiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    z= np.exp(z-z.max(axis=-1,keepdims=True))\n",
        "    z= z/z.sum(axis=-1,keepdims=True) \n",
        "    return z"
      ],
      "metadata": {
        "id": "zW--jMnq6xVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attntion(x,mask,W_QKV,W_out): \n",
        "    K,Q,V= np.split(x@W_QKV,3,axis= -1)\n",
        "    attn = softmax(K@Q.swapaxes(-1,-2)/np.sqrt(x.shape[-1]) +mask)\n",
        "    return attn@V@W_out,attn"
      ],
      "metadata": {
        "id": "StofYw4B62T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T,d = 100,64 \n",
        "pytorch_attn = nn.MultiheadAttention(d,1,bias=False,batch_first=True) \n",
        "M=torch.triu(-float(\"inf\")*torch.ones(T,T),1)\n",
        "X = torch.randn(1,T,d)\n",
        "Y_,A_ = pytorch_attn(X,X,X,attn_mask=M)"
      ],
      "metadata": {
        "id": "Xv__Env_62zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_attn.in_proj_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhMkjSlO64M5",
        "outputId": "04880437-4ff8-41fc-a680-b03aacbd43bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.0639, -0.1004,  0.1387,  ...,  0.0807, -0.0256,  0.0107],\n",
              "        [-0.1129, -0.0043,  0.0175,  ...,  0.0309,  0.1409, -0.0381],\n",
              "        [ 0.0396, -0.0262, -0.1473,  ..., -0.0444, -0.0025, -0.1250],\n",
              "        ...,\n",
              "        [-0.0588,  0.1344,  0.0573,  ..., -0.0036,  0.1160, -0.0110],\n",
              "        [ 0.0961,  0.1281,  0.0603,  ...,  0.1058, -0.0673,  0.1057],\n",
              "        [-0.1393,  0.0834,  0.0303,  ..., -0.0488, -0.0271,  0.0366]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pytorch_attn.in_proj_weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWC2IOys66ws",
        "outputId": "96ade6ad-2b38-4b81-dfaa-fef40ebd7808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([192, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y,a = self_attntion(X[0].numpy(),\n",
        "              M.detach().numpy(),\n",
        "              pytorch_attn.in_proj_weight.detach().numpy().T,\n",
        "              pytorch_attn.out_proj.weight.detach().numpy().T)"
      ],
      "metadata": {
        "id": "H723RMzc69oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(y-Y_.detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUO8XA5y6_84",
        "outputId": "e51c0274-d771-4133-f83d-b451fca067c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.1186613e-06"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### needle "
      ],
      "metadata": {
        "id": "G8J-I99a7GIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import needle as ndl\n",
        "from needle import backend_ndarray as nd"
      ],
      "metadata": {
        "id": "xwZaIo9h7hpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(4,5,3)\n",
        "y = np.random.rand(4,3,7)\n",
        "z = x@y \n",
        "z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqJFXyPjle-o",
        "outputId": "f3e42e95-1a40-4be0-bdd8-d7a424e796ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 5, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = ndl.Tensor(nd.array(x), device=ndl.cpu())\n"
      ],
      "metadata": {
        "id": "ska1B-Vy7sUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYrs1SdiWhQW",
        "outputId": "35dbf7c0-9d3a-4e3d-a02d-2b7cadba747c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "needle.autograd.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_= []\n",
        "list_+= [A]\n",
        "list_+= [A]\n",
        "list_+= [A]\n",
        "ndl.stack(list_,axis= 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P45FdSdT6k4",
        "outputId": "f46542ef-4a9b-4bea-b983-26b9d6f9a303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "needle.Tensor([[[[0.83470535 0.2566686  0.6160921 ]\n",
              "   [0.84934247 0.23257907 0.3750166 ]\n",
              "   [0.69505304 0.36830238 0.00763387]\n",
              "   [0.04668795 0.64148545 0.41045737]\n",
              "   [0.45124054 0.23824914 0.04564811]]\n",
              "\n",
              "  [[0.01622594 0.18151471 0.2542918 ]\n",
              "   [0.09291264 0.8217102  0.15739997]\n",
              "   [0.18919791 0.57033896 0.21660209]\n",
              "   [0.7835239  0.93566793 0.28094074]\n",
              "   [0.41302624 0.25173417 0.07278667]]\n",
              "\n",
              "  [[0.27279794 0.3372212  0.07701581]\n",
              "   [0.02709366 0.16226222 0.65644914]\n",
              "   [0.20317753 0.80822873 0.07684939]\n",
              "   [0.28478497 0.40078703 0.18103081]\n",
              "   [0.23169583 0.9919206  0.42142302]]\n",
              "\n",
              "  [[0.33048588 0.25607657 0.47141388]\n",
              "   [0.91839296 0.2121399  0.41372746]\n",
              "   [0.80201006 0.4352404  0.58073837]\n",
              "   [0.8987972  0.8565136  0.97653604]\n",
              "   [0.9503554  0.27668574 0.28438362]]]\n",
              "\n",
              "\n",
              " [[[0.83470535 0.2566686  0.6160921 ]\n",
              "   [0.84934247 0.23257907 0.3750166 ]\n",
              "   [0.69505304 0.36830238 0.00763387]\n",
              "   [0.04668795 0.64148545 0.41045737]\n",
              "   [0.45124054 0.23824914 0.04564811]]\n",
              "\n",
              "  [[0.01622594 0.18151471 0.2542918 ]\n",
              "   [0.09291264 0.8217102  0.15739997]\n",
              "   [0.18919791 0.57033896 0.21660209]\n",
              "   [0.7835239  0.93566793 0.28094074]\n",
              "   [0.41302624 0.25173417 0.07278667]]\n",
              "\n",
              "  [[0.27279794 0.3372212  0.07701581]\n",
              "   [0.02709366 0.16226222 0.65644914]\n",
              "   [0.20317753 0.80822873 0.07684939]\n",
              "   [0.28478497 0.40078703 0.18103081]\n",
              "   [0.23169583 0.9919206  0.42142302]]\n",
              "\n",
              "  [[0.33048588 0.25607657 0.47141388]\n",
              "   [0.91839296 0.2121399  0.41372746]\n",
              "   [0.80201006 0.4352404  0.58073837]\n",
              "   [0.8987972  0.8565136  0.97653604]\n",
              "   [0.9503554  0.27668574 0.28438362]]]\n",
              "\n",
              "\n",
              " [[[0.83470535 0.2566686  0.6160921 ]\n",
              "   [0.84934247 0.23257907 0.3750166 ]\n",
              "   [0.69505304 0.36830238 0.00763387]\n",
              "   [0.04668795 0.64148545 0.41045737]\n",
              "   [0.45124054 0.23824914 0.04564811]]\n",
              "\n",
              "  [[0.01622594 0.18151471 0.2542918 ]\n",
              "   [0.09291264 0.8217102  0.15739997]\n",
              "   [0.18919791 0.57033896 0.21660209]\n",
              "   [0.7835239  0.93566793 0.28094074]\n",
              "   [0.41302624 0.25173417 0.07278667]]\n",
              "\n",
              "  [[0.27279794 0.3372212  0.07701581]\n",
              "   [0.02709366 0.16226222 0.65644914]\n",
              "   [0.20317753 0.80822873 0.07684939]\n",
              "   [0.28478497 0.40078703 0.18103081]\n",
              "   [0.23169583 0.9919206  0.42142302]]\n",
              "\n",
              "  [[0.33048588 0.25607657 0.47141388]\n",
              "   [0.91839296 0.2121399  0.41372746]\n",
              "   [0.80201006 0.4352404  0.58073837]\n",
              "   [0.8987972  0.8565136  0.97653604]\n",
              "   [0.9503554  0.27668574 0.28438362]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python dummy_test.py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BB-2aVqRW5g",
        "outputId": "2e92ba1b-e886-4250-b3be-112ec1726b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "after numpy the shape is (4, 5, 7)\n",
            "A <class 'needle.autograd.Tensor'>\n",
            "<class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "<class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "<class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "<class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "c shape is (5, 7),type <class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "c shape is (5, 7),type <class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "c shape is (5, 7),type <class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "c shape is (5, 7),type <class 'needle.backend_ndarray.ndarray.NDArray'>\n",
            "length of output 4\n",
            "after numpy the shape is (4, 5, 7)\n",
            "3.648436804215749e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pytest -l -v -k \"bbm_forward\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdtpuKRCrGAN",
        "outputId": "2186d9b1-e1b1-44ef-c6bd-ce56d4258f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.8.16, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/hw4_, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1805 items / 1803 deselected                                         \u001b[0m\n",
            "\n",
            "tests/test_ext_ops.py::test_bbm_forward[params0-device0] \u001b[32mPASSED\u001b[0m\u001b[36m          [ 50%]\u001b[0m\n",
            "tests/test_ext_ops.py::test_bbm_forward[params0-device1] \u001b[32mPASSED\u001b[0m\u001b[36m          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m\u001b[1m================== 2 passed, 1803 deselected in 2.49 seconds ===================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = ndl.Tensor(nd.array(x), device=ndl.cpu())\n",
        "B = ndl.Tensor(nd.array(y), device=ndl.cpu())\n",
        "z2 = ndl.bmm(A,B)\n",
        "z2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "FosKUkabmEN0",
        "outputId": "e3d38a50-51cf-4f10-d1b6-c0389c5e4239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-60831207aa44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mz2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mndl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/ops.py\u001b[0m in \u001b[0;36mbmm\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mBMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/autograd.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_from_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/autograd.py\u001b[0m in \u001b[0;36mmake_from_op\u001b[0;34m(op, inputs)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize_cached_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/autograd.py\u001b[0m in \u001b[0;36mrealize_cached_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# note: data implicitly calls realized cached data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         self.cached_data = self.op.compute(\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize_cached_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         )\n",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/ops.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0moutput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0moutput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnew_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/backend_ndarray/ndarray.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idxs)\u001b[0m\n\u001b[1;32m    392\u001b[0m             ]\n\u001b[1;32m    393\u001b[0m         )\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Need indexes equal to number of dimensions\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m### BEGIN YOUR SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: Need indexes equal to number of dimensions"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[1,:,:].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wy__a7yT-rM",
        "outputId": "d7dd8546-7ed4-40b8-bb09-addb1fba38df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(K@Q.swapaxes(-1,-2)/np.sqrt(x.shape[-1]) +mask)"
      ],
      "metadata": {
        "id": "3JEU9Ar0MZuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8QKO5LCNO-_",
        "outputId": "4a745380-d597-4b70-be11-905e858f07fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([100, 64])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z1= K@Q.swapaxes(-1,-2)\n",
        "Z2 = ndl.Tensor(nd.array(K),device = ndl.cpu()) @ ndl.Tensor(nd.array(Q),device = ndl.cpu()).transpose((-1,-2))\n",
        "Z1= softmax(Z1.numpy())\n",
        "#Z2 = ndl.logsumexp(Z2,axes = -1)\n",
        "np.linalg.norm(Z2.numpy() - Z1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KlulMjTMijK",
        "outputId": "2be3b8f7-1d45-4c0c-f772-7ee2e46e9975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "389.3541"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert 1==2 , \"ass\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "292fOws5OkMi",
        "outputId": "41e10e15-b00e-413f-8991-833883a6322a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1873bb8af5c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"ass\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: ass"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def self_attntion_needle(x,mask,K,Q,V,W_out): \n",
        "    #K,Q,V= ndl.split(x@W_QKV,axis= -1)\n",
        "    interm = K@Q.transpose((-1,-2))\n",
        "    attn = ndl.exp(ndl.logsumexp(interm/np.sqrt(x.shape[-1]) +mask,axes=-1))\n",
        "    return attn@V@W_out,attn"
      ],
      "metadata": {
        "id": "Kpdqnpkv7Bnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K,Q,V = np.split(X[0]@pytorch_attn.in_proj_weight.detach().numpy().T,3,-1)"
      ],
      "metadata": {
        "id": "fac0Nc_lHWe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y1,a1= self_attntion_needle(ndl.Tensor(nd.array(X[0]), device=ndl.cpu()),\n",
        "                          ndl.Tensor(nd.array(M.detach()), device=ndl.cpu()),\n",
        "                          ndl.Tensor(nd.array(K), device=ndl.cpu()),\n",
        "                          ndl.Tensor(nd.array(Q), device=ndl.cpu()),\n",
        "                          ndl.Tensor(nd.array(V), device=ndl.cpu()),\n",
        "                          ndl.Tensor(nd.array(pytorch_attn.out_proj.weight.detach().numpy().T), device=ndl.cpu()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnWJY0i-8-cC",
        "outputId": "838f065c-151f-4236-92ac-78e7f11e54b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inter sshape (100, 100) , mask (100, 100), x \n",
            "am here 3 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(y1.numpy()-Y_.detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptYMXrvL-pTs",
        "outputId": "f33a3aee-9549-42f9-af74-426b9651f437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19494.152"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_.detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J58Sj-0-95g",
        "outputId": "8dd3350b-d34b-40a7-d041-18dd828ec11f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 1.4319317 ,  1.1891581 , -0.6933644 , ...,  0.65494126,\n",
              "          0.40657172,  0.52924234],\n",
              "        [ 0.7774052 ,  0.5540716 , -0.4839186 , ...,  0.43025815,\n",
              "          0.04043669,  0.47467348],\n",
              "        [ 0.6789432 ,  0.5909815 , -0.27973345, ...,  0.29926848,\n",
              "         -0.13691753,  0.467305  ],\n",
              "        ...,\n",
              "        [ 0.006937  , -0.01924795,  0.01213193, ..., -0.03500503,\n",
              "          0.06926697, -0.01781674],\n",
              "        [ 0.09237896,  0.08156796, -0.03369059, ..., -0.00229773,\n",
              "          0.08049973, -0.04902685],\n",
              "        [ 0.06407653, -0.00245799,  0.06508794, ..., -0.02389152,\n",
              "          0.0328558 ,  0.00353461]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y1.numpy() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CCURVSVClOk",
        "outputId": "67030b73-a1ee-4407-f097-587e2ca2b335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.02142000e+00, -2.31199121e+00,  2.98760056e+00, ...,\n",
              "        -1.40650272e+00,  9.30363560e+00, -1.43880665e+00],\n",
              "       [ 1.82752724e+01, -1.16519232e+01,  1.37074995e+01, ...,\n",
              "        -6.65031195e+00,  4.59798355e+01, -8.53326416e+00],\n",
              "       [ 2.05431976e+01, -1.69026840e+00,  5.93793869e+00, ...,\n",
              "        -7.12287605e-01,  3.52013893e+01, -2.90395403e+00],\n",
              "       ...,\n",
              "       [ 3.91363647e+02, -1.41149582e+02,  3.09109863e+02, ...,\n",
              "        -2.29007301e+01,  6.58865295e+02, -1.57089462e+01],\n",
              "       [ 5.21057068e+02, -1.57911133e+02,  1.93444427e+02, ...,\n",
              "        -1.15784973e+02,  9.66197388e+02, -1.45222260e+02],\n",
              "       [ 2.85343384e+02, -2.74345541e+00, -7.62693939e+01, ...,\n",
              "        -8.90989151e+01,  5.12652954e+02, -6.94488754e+01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Split"
      ],
      "metadata": {
        "id": "KIzMwpfwTzjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python dummy_test.py  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5qKStUtT1KS",
        "outputId": "c65ed094-15ac-4dfd-f708-0c04162b8484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "am here 0\n",
            "am here 1 \n",
            "am here 2 \n",
            "arr shape here is (15, 108) an new sshape (15, 4, 9, 3), fially axes [2, 0, 1, 3]\n",
            "5.361648239661069e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "wZ05qGWnT9Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= np.random.rand(5,7,8,9)\n"
      ],
      "metadata": {
        "id": "kUwQd4-ncx1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import needle.nn as nn "
      ],
      "metadata": {
        "id": "6ReKhmYYd9xf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= np.random.rand(10,5,4,9)\n",
        "y= softmax(x)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GayvjPcCkDfy",
        "outputId": "4a4fafc0-3627-458d-8fc1-b1a64a2dc788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5, 4, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = nn.Softmax()(ndl.Tensor(nd.array(x), device=ndl.cpu()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "N707W7dPkPcK",
        "outputId": "f2fe024e-5689-4a99-952e-1809a346cabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-e6659e39a2da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/nn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/10714/hw4_/./python/needle/nn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0;31m### BEGIN YOUR SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mx_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'max'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y= ndl.Tensor(nd.array(x), device=ndl.cpu())"
      ],
      "metadata": {
        "id": "YGKl3iPoke-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iysEwzetdv-H",
        "outputId": "39a04063-dd1c-4d31-d194-637a462c9e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 7, 8, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2= ndl.logsumexp(y,axes= 3)\n",
        "l2 = l2.reshape((5, 7, 8, 1))\n",
        "l2= l2.broadcast_to((5, 7, 8, 9))\n",
        "softmax_ = ndl.exp(y - l2)\n"
      ],
      "metadata": {
        "id": "BjkhEFARHon_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O9p6knK2d3rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z1 = softmax(y.numpy())"
      ],
      "metadata": {
        "id": "ZV8bZMEyN9JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(softmax_.numpy() - z1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1gNUtpuOOYr",
        "outputId": "2b88e334-3d79-4101-aa73-fc84a67309f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.349498e-07"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy"
      ],
      "metadata": {
        "id": "ngcntfAaOTIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = logsumexp(y.numpy(), axis=-1, keepdims=True)\n",
        "z2 = np.exp(y.numpy() - l1)"
      ],
      "metadata": {
        "id": "6-kIJdDqQCbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import logsumexp\n"
      ],
      "metadata": {
        "id": "IlP4g6MzQ4ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaW6J5ShZF0-",
        "outputId": "7de861e0-62d9-4693-f106-a235a6cbb117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 7, 8, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = logsumexp(y.numpy(), axis=3, keepdims=True)\n",
        "l2 = ndl.logsumexp(y,axes =  3)"
      ],
      "metadata": {
        "id": "YWfKmhldR1R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(l2.shape)\n",
        "print(l1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4ycRM-VZLYg",
        "outputId": "5f5ba27b-0081-4793-8677-fdb432c4224d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 7, 8)\n",
            "(5, 7, 8, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l2 = l2.reshape((5, 7, 8, 1))\n"
      ],
      "metadata": {
        "id": "D7_CqHRWdGzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.linalg.norm(l2.numpy()-l1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eafg-OlPSPRS",
        "outputId": "e21eb752-587a-463e-e278-c9d7adac2d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7357135e-06"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.logsumexp(A_t, dim=t_axes).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZfYI-XkSRx-",
        "outputId": "6a9a70db-7f04-4561-d468-94865c10ebd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.8.16, pytest-3.6.4, py-1.11.0, pluggy-0.7.1 -- /usr/bin/python3\n",
            "cachedir: .pytest_cache\n",
            "rootdir: /content/drive/MyDrive/10714/hw4_, inifile:\n",
            "plugins: typeguard-2.7.1\n",
            "collected 1805 items / 1687 deselected                                         \u001b[0m\n",
            "\n",
            "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[36m        [  0%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[36m      [  1%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[36m        [  2%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[36m      [  3%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[36m       [  4%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[36m     [  5%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[36m       [  5%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[36m     [  6%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[36m       [  7%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[36m     [  8%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[36m       [  9%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[36m     [ 10%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[36m      [ 11%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[36m    [ 11%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[36m      [ 12%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[36m    [ 13%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 14%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 15%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 16%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 16%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 17%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 18%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 19%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 20%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 21%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[36m               [ 22%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[36m            [ 22%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[31mFAILED\u001b[0m\u001b[36m              [ 23%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[31mFAILED\u001b[0m\u001b[36m                 [ 24%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[31mFAILED\u001b[0m\u001b[36m                 [ 25%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[31mFAILED\u001b[0m\u001b[36m                 [ 26%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[31mFAILED\u001b[0m\u001b[36m                 [ 27%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[31mFAILED\u001b[0m\u001b[36m              [ 27%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[31mFAILED\u001b[0m\u001b[36m              [ 28%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[31mFAILED\u001b[0m\u001b[36m              [ 29%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[31mFAILED\u001b[0m\u001b[36m              [ 30%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[31mFAILED\u001b[0m\u001b[36m              [ 31%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[31mFAILED\u001b[0m\u001b[36m           [ 32%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 33%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m                  [ 33%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_power[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m                 [ 34%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_power[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m                 [ 35%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m                    [ 36%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m                    [ 37%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_log[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m                   [ 38%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_log[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m                   [ 38%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m                    [ 39%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m                    [ 40%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_exp[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m                   [ 41%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_exp[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m                   [ 42%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m                   [ 43%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m                   [ 44%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_relu[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m                  [ 44%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_relu[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m                  [ 45%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m                   [ 46%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m                   [ 47%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m                  [ 48%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m                  [ 49%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m          [ 50%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m          [ 50%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m         [ 51%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m         [ 52%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[36m              [ 53%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[36m              [ 54%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[36m              [ 55%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[36m             [ 55%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[36m             [ 56%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[36m             [ 57%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[36m     [ 58%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[36m     [ 59%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[36m     [ 60%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[36m    [ 61%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[36m    [ 61%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[36m    [ 62%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[36m         [ 63%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[36m            [ 64%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[36m            [ 65%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[36m            [ 66%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[36m        [ 66%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[36m           [ 67%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[36m           [ 68%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[36m           [ 69%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[36m [ 70%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 71%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 72%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[36m   [ 72%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[36m [ 73%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[36m  [ 74%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[36m  [ 75%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[36m  [ 76%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[36m [ 77%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[36m [ 77%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[36m [ 78%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[36m [ 79%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[36m      [ 80%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[36m      [ 81%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[36m     [ 82%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[36m     [ 83%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m        [ 83%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m        [ 84%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m        [ 85%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m        [ 86%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[36m         [ 87%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[36m         [ 88%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m       [ 88%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m       [ 89%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m       [ 90%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m       [ 91%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[36m        [ 92%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[36m        [ 93%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[36m         [ 94%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[36m            [ 94%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[36m            [ 95%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[36m            [ 96%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[36m        [ 97%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[36m           [ 98%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[36m           [ 99%]\u001b[0m\n",
            "tests/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[36m           [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape0-divide] _______________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe84116a9d0>, shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_ewise_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.34836832]]], dtype=float32)\n",
            "_B         = array([[[-1.8110192]]], dtype=float32)\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe84116a9d0>\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:54: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84063b8e0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape0-subtract] ______________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe840ce3310>, shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_ewise_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[0.9578964]]], dtype=float32)\n",
            "_B         = array([[[1.2263213]]], dtype=float32)\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe840ce3310>\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:54: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840520c40>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape1-divide] _______________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe84116a9d0>, shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_ewise_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-1.4833167 ,  0.13791604,  1.1932526 , -1.0757235 ,\n",
            "          1.7676828 , -0.34903246],\n",
            "        [-1.0753901 ,...838],\n",
            "        [ 0.11024354,  0.57954335, -1.7123466 ,  0.42849943,\n",
            "          0.9961082 ,  0.33900097]]], dtype=float32)\n",
            "_B         = array([[[ 0.14525874, -0.9536581 ,  0.6435548 ,  0.20046894,\n",
            "          0.00906379, -0.56320494],\n",
            "        [ 0.85617113,...21 ],\n",
            "        [-1.350194  ,  1.6009326 ,  0.2970727 , -0.2077485 ,\n",
            "         -1.8044882 ,  1.087597  ]]], dtype=float32)\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe84116a9d0>\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:54: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84046cdc0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape1-subtract] ______________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe840ce3310>, shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_ewise_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.96954954, -0.5111908 ,  0.5459043 , -1.0255798 ,\n",
            "         -0.9868334 , -1.1512803 ],\n",
            "        [-0.11136018,...78 ],\n",
            "        [ 0.07969699,  0.35053274, -0.38154238, -0.9728293 ,\n",
            "          0.06547206,  0.1765085 ]]], dtype=float32)\n",
            "_B         = array([[[-0.6627614 ,  1.8296949 , -1.6159147 , -0.8153447 ,\n",
            "         -0.6713712 ,  1.2912054 ],\n",
            "        [-0.2565905 ,...04 ],\n",
            "        [ 0.6192242 , -0.14435019, -0.21653645,  0.7103066 ,\n",
            "         -0.47707722, -1.3997524 ]]], dtype=float32)\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe840ce3310>\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:54: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8404aebb0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape0-divide] ______________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe840ce3700>, shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_scalar_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(1).astype(np.float32).item()\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.21264285]]], dtype=float32)\n",
            "_B         = -0.6414637565612793\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe840ce3700>\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:71: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403ca370>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_scalar_fn[cuda-shape0-subtract] _____________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe840ce3790>, shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_scalar_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(1).astype(np.float32).item()\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[0.7919234]]], dtype=float32)\n",
            "_B         = -0.7252244353294373\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe840ce3790>\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:71: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8c1ea5340>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape1-divide] ______________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe840ce3700>, shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_scalar_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(1).astype(np.float32).item()\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 0.17852722, -0.48561108,  0.4178631 ,  0.6601184 ,\n",
            "         -0.26862556,  0.04397374],\n",
            "        [ 0.11434969,...873],\n",
            "        [ 0.95702404,  0.3420432 , -1.1888571 , -1.3259709 ,\n",
            "         -1.4491868 ,  0.3708898 ]]], dtype=float32)\n",
            "_B         = -0.7234068512916565\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe840ce3700>\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:71: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8c1eae8e0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_scalar_fn[cuda-shape1-subtract] _____________________\u001b[0m\n",
            "\n",
            "fn = <function <lambda> at 0x7fe840ce3790>, shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"fn\", SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_scalar_fn(fn, shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(1).astype(np.float32).item()\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.37743163, -0.0939057 ,  1.2136433 ,  2.1777341 ,\n",
            "         -0.12863235,  1.989526  ],\n",
            "        [ 0.5402597 ,...386],\n",
            "        [-1.2373476 ,  0.6722726 , -1.3963248 ,  2.26083   ,\n",
            "         -0.72201365, -0.01691202]]], dtype=float32)\n",
            "_B         = 1.3843916654586792\n",
            "device     = cuda()\n",
            "fn         = <function <lambda> at 0x7fe840ce3790>\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:71: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8405e74c0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-16-16-16] __________________________\u001b[0m\n",
            "\n",
            "m = 16, n = 16, p = 16, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-4.8341087e-01,  1.8093601e+00,  1.6722593e+00,  8.0163997e-01,\n",
            "         1.5130348e+00,  5.7170558e-01, -6.382...793e+00, -1.2557693e-01,\n",
            "         1.1033653e+00,  9.9985802e-01,  1.3731679e+00, -6.6210198e-01]],\n",
            "      dtype=float32)\n",
            "_B         = array([[-0.6730201 , -1.2317069 ,  0.6105233 , -1.0086207 ,  0.971255  ,\n",
            "        -0.98045826, -0.87343013, -0.50210905...690245 ,\n",
            "         1.0893643 , -1.0063945 , -0.397469  , -1.5228411 ,  0.27464268,\n",
            "         0.8954157 ]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 16\n",
            "n          = 16\n",
            "p          = 16\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (16, 16), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840481af0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (16, 16)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-8-8-8] ____________________________\u001b[0m\n",
            "\n",
            "m = 8, n = 8, p = 8, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-1.4774635 , -1.6267474 ,  0.49602756,  0.31081805,  1.2617579 ,\n",
            "         0.89151007, -0.14909866, -1.3068166 ...33,  1.380418  , -1.8872807 ,  0.7998044 , -1.1721367 ,\n",
            "        -0.11657883,  1.6036352 , -1.1328816 ]], dtype=float32)\n",
            "_B         = array([[ 1.0537726 , -0.19507441,  0.6123048 , -1.2122457 ,  0.46485847,\n",
            "         0.58945036,  2.0647578 ,  1.6958039 ...4 ,  1.4154483 , -1.5804107 , -0.13925153,  0.88675374,\n",
            "         1.7259799 , -0.5968434 ,  2.8232105 ]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 8\n",
            "n          = 8\n",
            "p          = 8\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (8, 8), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840516c40>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (8, 8)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-1-2-3] ____________________________\u001b[0m\n",
            "\n",
            "m = 1, n = 2, p = 3, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-0.0460759, -0.299862 ]], dtype=float32)\n",
            "_B         = array([[ 0.5341751 ,  1.3190286 ,  1.9854463 ],\n",
            "       [-0.8265349 ,  0.05200673, -2.2706025 ]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 1\n",
            "n          = 2\n",
            "p          = 3\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 2), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8404e4340>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 2)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-3-4-5] ____________________________\u001b[0m\n",
            "\n",
            "m = 3, n = 4, p = 5, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[ 0.654816  ,  0.72850287, -0.2106692 , -0.17719436],\n",
            "       [ 0.85219604,  0.45814183, -0.09138661, -1.1754771 ],\n",
            "       [-0.23682164,  1.7254593 ,  0.7422256 , -0.0074945 ]],\n",
            "      dtype=float32)\n",
            "_B         = array([[-0.3631913 , -0.8601258 ,  0.3114279 ,  0.5743038 ,  0.24980997],\n",
            "       [ 0.6041445 ,  1.287218  , -0.0696068...1749268 , -1.5551547 ],\n",
            "       [ 0.62649083,  0.12255095, -1.0435718 ,  0.24988598,  0.3234629 ]],\n",
            "      dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 3\n",
            "n          = 4\n",
            "p          = 5\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (3, 4), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403d83d0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (3, 4)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________________ test_matmul[cuda-5-4-3] ____________________________\u001b[0m\n",
            "\n",
            "m = 5, n = 4, p = 3, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-1.3892012 , -1.4697326 ,  1.0070543 ,  0.34883013],\n",
            "       [-2.0843477 , -0.24267545,  0.11248029,  1.987426 ...0366048 , -1.0864305 ,  0.47813606],\n",
            "       [-0.03617776,  0.03262429, -1.8234262 , -0.15546963]],\n",
            "      dtype=float32)\n",
            "_B         = array([[ 0.7341658 ,  0.8313993 ,  0.08911297],\n",
            "       [-0.10103321,  0.25325453,  0.22029683],\n",
            "       [-1.2978022 , -0.55815214,  1.5079324 ],\n",
            "       [ 0.19379255, -0.96344835, -0.30928165]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 5\n",
            "n          = 4\n",
            "p          = 3\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 4), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84065ac40>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 4)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-16-16-32] __________________________\u001b[0m\n",
            "\n",
            "m = 16, n = 16, p = 32, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-5.92951536e-01, -8.68785977e-01,  1.28109825e+00,\n",
            "        -2.13154539e-01,  1.37239945e+00, -3.68608564e-01,\n",
            "..., -2.11009383e+00,\n",
            "        -2.48701707e-01, -7.93531001e-01,  2.41460130e-01,\n",
            "        -4.81520534e-01]], dtype=float32)\n",
            "_B         = array([[-7.37000585e-01,  1.07231128e+00, -1.86253166e+00,\n",
            "        -8.23480904e-01,  1.96635008e-01, -1.68239748e+00,\n",
            "...,\n",
            "        -1.90087244e-01, -8.86591554e-01,  1.40486643e-01,\n",
            "        -8.94032776e-01,  2.30349619e-02]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 16\n",
            "n          = 16\n",
            "p          = 32\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (16, 16), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84051a9d0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (16, 16)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-64-64-64] __________________________\u001b[0m\n",
            "\n",
            "m = 64, n = 64, p = 64, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-1.082387  , -0.31285337,  0.37914652, ..., -0.41516542,\n",
            "         1.9055135 , -1.2175403 ],\n",
            "       [-0.4006369...9],\n",
            "       [-0.6071943 ,  0.6906254 , -0.8257664 , ..., -0.33885276,\n",
            "         0.4185762 , -1.6399068 ]], dtype=float32)\n",
            "_B         = array([[ 0.41004443,  1.1745555 ,  0.80214846, ..., -0.5857207 ,\n",
            "         0.7658256 ,  0.8750583 ],\n",
            "       [ 1.1478539... ],\n",
            "       [ 0.11291716,  0.47895536, -1.1013322 , ...,  0.8595541 ,\n",
            "        -1.5817857 ,  0.8475244 ]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 64\n",
            "n          = 64\n",
            "p          = 64\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (64, 64), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403a64c0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (64, 64)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-72-72-72] __________________________\u001b[0m\n",
            "\n",
            "m = 72, n = 72, p = 72, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-0.14028323,  0.60604614,  0.04811767, ...,  0.51525635,\n",
            "         0.45924923,  1.9541698 ],\n",
            "       [-1.1255426... ],\n",
            "       [-0.13650139,  0.41142735, -1.6502085 , ..., -2.5761743 ,\n",
            "         1.4478314 ,  2.2190225 ]], dtype=float32)\n",
            "_B         = array([[ 0.24280068,  0.02310721,  1.9812624 , ..., -1.1015782 ,\n",
            "        -0.3129008 ,  0.5849371 ],\n",
            "       [ 1.4762851... ],\n",
            "       [ 0.76414275,  0.25707817, -2.0578747 , ...,  0.6905931 ,\n",
            "        -1.3811514 ,  0.6022044 ]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 72\n",
            "n          = 72\n",
            "p          = 72\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (72, 72), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403e0070>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (72, 72)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-72-73-74] __________________________\u001b[0m\n",
            "\n",
            "m = 72, n = 73, p = 74, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-2.2869356 ,  2.4410696 , -0.06609856, ..., -0.6628585 ,\n",
            "         1.2714901 , -1.6431442 ],\n",
            "       [-0.6135641... ],\n",
            "       [ 0.37932992,  0.2737079 ,  1.8354076 , ...,  0.7942683 ,\n",
            "         2.1143708 ,  0.5757829 ]], dtype=float32)\n",
            "_B         = array([[-0.81970483, -0.62832344,  0.58655894, ..., -0.7188211 ,\n",
            "        -0.16015641, -0.38464722],\n",
            "       [-0.9847221... ],\n",
            "       [-0.8729308 , -0.37538484,  0.90310943, ...,  0.34306654,\n",
            "        -0.10175896,  1.9507737 ]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 72\n",
            "n          = 73\n",
            "p          = 74\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (72, 73), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84036e280>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (72, 73)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________________ test_matmul[cuda-74-73-72] __________________________\u001b[0m\n",
            "\n",
            "m = 74, n = 73, p = 72, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[ 2.5433717 ,  2.202355  , -1.7542894 , ...,  0.4746837 ,\n",
            "         0.291169  ,  0.632095  ],\n",
            "       [ 0.5023340...8],\n",
            "       [ 0.3124429 ,  0.67045975, -0.2541804 , ...,  0.8254428 ,\n",
            "        -0.9933159 , -1.3417903 ]], dtype=float32)\n",
            "_B         = array([[ 0.86880785, -0.70741963, -0.14541002, ...,  0.33905905,\n",
            "         0.03319501, -0.30614465],\n",
            "       [ 0.3715162...3],\n",
            "       [ 1.4325385 ,  0.01491717, -0.4534368 , ..., -1.5561616 ,\n",
            "        -0.32822824, -0.85069335]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 74\n",
            "n          = 73\n",
            "p          = 72\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (74, 73), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840566280>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (74, 73)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m________________________ test_matmul[cuda-128-128-128] _________________________\u001b[0m\n",
            "\n",
            "m = 128, n = 128, p = 128, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"m,n,p\", MATMUL_DIMS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_matmul(m, n, p, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(m, n).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randn(n, p).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[ 1.9537501 ,  0.05903736, -1.2711045 , ...,  0.22699478,\n",
            "        -0.10282926, -0.18651716],\n",
            "       [-0.2311179...2],\n",
            "       [ 0.06558133, -1.1310216 , -1.9421903 , ..., -1.8011059 ,\n",
            "         1.3072039 ,  0.5935067 ]], dtype=float32)\n",
            "_B         = array([[ 0.9572396 , -0.46439824, -1.6827878 , ..., -0.11662703,\n",
            "        -1.5686997 ,  0.01031934],\n",
            "       [-0.1855606...7],\n",
            "       [-0.8687678 , -1.4076406 ,  2.1396523 , ...,  1.6590946 ,\n",
            "        -0.4116172 , -0.4557549 ]], dtype=float32)\n",
            "device     = cuda()\n",
            "m          = 128\n",
            "n          = 128\n",
            "p          = 128\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:91: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (128, 128), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840398220>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (128, 128)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________________ test_power[cuda-shape0] ____________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_power(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randint(1)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[1.171145]]], dtype=float32)\n",
            "_B         = 0\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:101: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840572430>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________________ test_power[cuda-shape1] ____________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_power(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m        _B = np.random.randint(1)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.6280028 , -0.43987197,  2.9139524 ,  0.05009432,\n",
            "         -0.73577106, -0.47775924],\n",
            "        [-1.7431105 ,...16 ],\n",
            "        [-0.8517918 , -1.9387237 ,  0.42015556, -0.24831858,\n",
            "         -1.0194412 ,  0.38210192]]], dtype=float32)\n",
            "_B         = 0\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:101: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840576970>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_log[cuda-shape0] _____________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_log(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32) + 5.\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[6.5344124]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:109: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8405b2cd0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_log[cuda-shape1] _____________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_log(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32) + 5.\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[3.516722 , 4.4941716, 3.9078727, 5.770175 , 4.984753 ,\n",
            "         5.790717 ],\n",
            "        [2.6877737, 4.846334 , 5....      5.082136 ],\n",
            "        [6.005202 , 4.3703594, 5.3648767, 4.616481 , 5.4491186,\n",
            "         5.858667 ]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:109: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8404c4dc0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_exp[cuda-shape0] _____________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_exp(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[0.14809337]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:117: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8405c5490>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_exp[cuda-shape1] _____________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_exp(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 2.4905684 , -0.6954556 , -1.8243234 , -0.31614092,\n",
            "          0.08606078, -0.74876255],\n",
            "        [ 1.3998232 ,...898],\n",
            "        [ 0.1688959 ,  0.02774058, -1.563431  ,  0.17357937,\n",
            "          0.47140115,  1.1728562 ]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:117: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403a1eb0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_relu[cuda-shape0] ____________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_relu(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[1.2374331]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:125: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8405de040>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_relu[cuda-shape1] ____________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_relu(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.05377588,  0.20338777,  0.16131176,  0.04317565,\n",
            "         -1.4132142 ,  0.14171205],\n",
            "        [-0.5501977 ,...069],\n",
            "        [-0.79452735,  0.3885921 ,  0.13083026,  0.47227657,\n",
            "         -2.1078956 ,  0.21374775]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:125: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8c1ea9280>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape0] ____________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_tanh(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-2.3530958]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:133: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8404a5520>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape1] ____________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_tanh(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.5291037 , -1.1316243 ,  1.2829843 ,  0.6779355 ,\n",
            "         -0.32166547, -0.51734763],\n",
            "        [ 0.48438653,...48 ],\n",
            "        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n",
            "          0.99664706, -1.344323  ]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:133: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84053b940>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape0] ________________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_tanh_backward(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[0.90795034]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:141: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840556d60>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape1] ________________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", GENERAL_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_tanh_backward(shape, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 1.1914672 ,  0.6276671 ,  1.7709465 ,  2.5802875 ,\n",
            "          1.0994289 , -0.25607443],\n",
            "        [-0.30351138,...125],\n",
            "        [ 0.6493112 , -0.8847228 , -1.9263881 , -1.6778411 ,\n",
            "          0.455592  ,  0.29220578]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:141: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840648a60>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape0-0-1] __________________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axis, l\", STACK_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_stack(shape, axis, l, device):\u001b[0m\n",
            "\u001b[1m        _A = [np.random.randn(*shape).astype(np.float32) for i in range(l)]\u001b[0m\n",
            "\u001b[1m>       A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\n",
            "_A         = [array([[ 1.7134967 , -0.41470623, -1.7430415 ,  0.6519293 ,  0.07126308],\n",
            "       [-0.48818034, -0.42512143, -0.232472...629536 ,  1.272113  ],\n",
            "       [-0.71344495, -1.5708133 , -0.37528518,  0.11450187,  1.4668971 ]],\n",
            "      dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cuda()\n",
            "l          = 1\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:152: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:152: in <listcomp>\n",
            "\u001b[1m    A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 5), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8c1ea61f0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 5)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape1-0-2] __________________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axis, l\", STACK_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_stack(shape, axis, l, device):\u001b[0m\n",
            "\u001b[1m        _A = [np.random.randn(*shape).astype(np.float32) for i in range(l)]\u001b[0m\n",
            "\u001b[1m>       A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\n",
            "_A         = [array([[-1.3115026 , -0.10614888,  0.6780119 , -0.74381435,  0.29154974],\n",
            "       [-1.0264622 ,  0.05647625,  1.332177...8355542,  1.1190343 ],\n",
            "       [-1.2428849 ,  0.34594685, -0.83825415,  0.0265399 , -0.13059294]],\n",
            "      dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cuda()\n",
            "l          = 2\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:152: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:152: in <listcomp>\n",
            "\u001b[1m    A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 5), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8402fd1f0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 5)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape2-2-5] __________________________\u001b[0m\n",
            "\n",
            "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axis, l\", STACK_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_stack(shape, axis, l, device):\u001b[0m\n",
            "\u001b[1m        _A = [np.random.randn(*shape).astype(np.float32) for i in range(l)]\u001b[0m\n",
            "\u001b[1m>       A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\n",
            "_A         = [array([[[-2.0257597e+00, -9.2441577e-01,  8.7577009e-01,  8.2509273e-01,\n",
            "         -3.4123892e-01,  1.1967411e+00, -2....[-1.4124417 ,  1.7589831 , -2.416149  ,  0.22257139,\n",
            "         -0.19093359, -1.1048661 ,  1.0865548 ]]], dtype=float32)]\n",
            "axis       = 2\n",
            "device     = cuda()\n",
            "l          = 5\n",
            "shape      = (1, 5, 7)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:152: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:152: in <listcomp>\n",
            "\u001b[1m    A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 5, 7), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8c1eab550>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 5, 7)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape0-0-1] _____________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 1, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axis, l\", STACK_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_stack_backward(shape, axis, l, device):\u001b[0m\n",
            "\u001b[1m        _A = [np.random.randn(*shape).astype(np.float32) for i in range(l)]\u001b[0m\n",
            "\u001b[1m>       A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\n",
            "_A         = [array([[ 0.5721981 ,  0.2235861 ,  0.35144737, -1.193306  , -0.37559596],\n",
            "       [-0.8369131 ,  1.9658917 , -0.515321...222518 ,  1.489412  ],\n",
            "       [ 1.1438326 ,  1.6791093 , -1.203218  , -1.4277622 , -0.7563902 ]],\n",
            "      dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cuda()\n",
            "l          = 1\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:163: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:163: in <listcomp>\n",
            "\u001b[1m    A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 5), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8404c0f10>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 5)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape1-0-2] _____________________\u001b[0m\n",
            "\n",
            "shape = (5, 5), axis = 0, l = 2, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axis, l\", STACK_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_stack_backward(shape, axis, l, device):\u001b[0m\n",
            "\u001b[1m        _A = [np.random.randn(*shape).astype(np.float32) for i in range(l)]\u001b[0m\n",
            "\u001b[1m>       A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\n",
            "_A         = [array([[-0.32789522,  1.3911211 ,  0.7655447 ,  1.9600631 ,  1.5961113 ],\n",
            "       [-0.2593512 , -0.8191968 , -0.009145...2705046,  0.07045578],\n",
            "       [ 0.05973172, -1.8943669 , -1.3532592 ,  0.08178733,  1.1173035 ]],\n",
            "      dtype=float32)]\n",
            "axis       = 0\n",
            "device     = cuda()\n",
            "l          = 2\n",
            "shape      = (5, 5)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:163: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:163: in <listcomp>\n",
            "\u001b[1m    A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 5), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403bf070>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 5)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape2-2-5] _____________________\u001b[0m\n",
            "\n",
            "shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axis, l\", STACK_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_stack_backward(shape, axis, l, device):\u001b[0m\n",
            "\u001b[1m        _A = [np.random.randn(*shape).astype(np.float32) for i in range(l)]\u001b[0m\n",
            "\u001b[1m>       A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\n",
            "_A         = [array([[[-1.9620842 ,  1.1954788 , -0.591674  ,  1.1645355 ,\n",
            "         -1.3027852 ,  1.5342665 ,  1.0155736 ],\n",
            "       ...[-0.31015334,  0.9066847 , -0.15136234,  0.8248054 ,\n",
            "          0.588381  ,  1.102897  ,  0.14859103]]], dtype=float32)]\n",
            "axis       = 2\n",
            "device     = cuda()\n",
            "l          = 5\n",
            "shape      = (1, 5, 7)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:163: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:163: in <listcomp>\n",
            "\u001b[1m    A = [ndl.Tensor(nd.array(_A[i]), device=device) for i in range(l)]\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 5, 7), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840654dc0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 5, 7)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_______________________ test_summation[cuda-shape0-None] _______________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-1.9483194]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:182: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403a9070>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape1-0] _________________________\u001b[0m\n",
            "\n",
            "shape = (5, 3), axes = 0, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[ 0.21689712,  0.43637753, -0.49504337],\n",
            "       [ 1.4166641 , -0.2632424 ,  1.5031598 ],\n",
            "       [-1.2881242 , -...3875 ],\n",
            "       [ 1.1767663 , -1.223357  ,  1.9838535 ],\n",
            "       [-0.40351707,  1.5878327 ,  1.8947744 ]], dtype=float32)\n",
            "axes       = 0\n",
            "device     = cuda()\n",
            "shape      = (5, 3)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:182: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 3), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8405e0070>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 3)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape2-1] _________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 1, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 1.2847524 , -0.955546  ],\n",
            "        [-0.42559817,  0.98641783],\n",
            "        [ 0.02986824, -0.71762425]],\n",
            "\n",
            "       [...  [[ 2.3362038 , -0.02063232],\n",
            "        [ 0.17829332,  1.2977082 ],\n",
            "        [ 1.223755  ,  0.5619403 ]]], dtype=float32)\n",
            "axes       = 1\n",
            "device     = cuda()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:182: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (8, 3, 2), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84064a250>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (8, 3, 2)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m________________________ test_summation[cuda-shape3-2] _________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 2, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 0.5748034 , -0.39962965],\n",
            "        [ 0.39787385, -1.5687171 ],\n",
            "        [-0.5583447 , -0.8579677 ]],\n",
            "\n",
            "       [...  [[-0.7648952 , -0.33858636],\n",
            "        [ 1.2390388 , -1.1854753 ],\n",
            "        [ 0.45743605,  1.3178569 ]]], dtype=float32)\n",
            "axes       = 2\n",
            "device     = cuda()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:182: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (8, 3, 2), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403686a0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (8, 3, 2)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________ test_summation_backward[cuda-shape0-None] ___________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation_backward(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[1.7799296]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:190: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84054cc70>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape1-0] ____________________\u001b[0m\n",
            "\n",
            "shape = (5, 3), axes = 0, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation_backward(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[ 2.1140475 , -0.9384094 , -0.2541468 ],\n",
            "       [ 1.4692454 , -0.2594365 ,  1.2197713 ],\n",
            "       [-0.9723478 , -...47044],\n",
            "       [ 0.16003066,  1.2728198 ,  1.1000712 ],\n",
            "       [-0.74093074, -0.50888014,  0.408108  ]], dtype=float32)\n",
            "axes       = 0\n",
            "device     = cuda()\n",
            "shape      = (5, 3)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:190: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 3), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840577640>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 3)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape2-1] ____________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 1, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation_backward(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 0.53634965, -2.1118162 ],\n",
            "        [-0.27859452,  1.0560247 ],\n",
            "        [-0.5948552 , -1.2022481 ]],\n",
            "\n",
            "       [...  [[-0.3477448 , -1.2089167 ],\n",
            "        [-0.03831517, -1.8689636 ],\n",
            "        [-0.53026855, -1.1635985 ]]], dtype=float32)\n",
            "axes       = 1\n",
            "device     = cuda()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:190: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (8, 3, 2), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840586ac0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (8, 3, 2)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape3-2] ____________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 2, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_summation_backward(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.6694544 ,  0.99575835],\n",
            "        [-0.675602  , -1.1355246 ],\n",
            "        [-0.3549894 ,  0.46968827]],\n",
            "\n",
            "       [...  [[-1.3204774 ,  2.5963671 ],\n",
            "        [-1.2905574 , -0.2102861 ],\n",
            "        [-0.09622368, -0.613245  ]]], dtype=float32)\n",
            "axes       = 2\n",
            "device     = cuda()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:190: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (8, 3, 2), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840369bb0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (8, 3, 2)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________ test_broadcast_to[cuda-shape0-shape_to0] ___________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), shape_to = (3, 3, 3), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape,shape_to\", BROADCAST_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_broadcast_to(shape, shape_to, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[0.45943186]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "shape_to   = (3, 3, 3)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:200: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe840586a60>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m___________________ test_broadcast_to[cuda-shape1-shape_to1] ___________________\u001b[0m\n",
            "\n",
            "shape = (4, 1, 6), shape_to = (4, 3, 6), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape,shape_to\", BROADCAST_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_broadcast_to(shape, shape_to, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 0.8649256 , -0.5685719 , -0.13818243,  0.02258228,\n",
            "          0.7159469 ,  1.4280046 ]],\n",
            "\n",
            "       [[ 0.0493846... ]],\n",
            "\n",
            "       [[-0.53128463,  1.3710403 , -0.17000628, -1.4833229 ,\n",
            "          0.02208569,  1.2701598 ]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (4, 1, 6)\n",
            "shape_to   = (4, 3, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:200: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 1, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8402f5070>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 1, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_reshape[cuda-shape0-shape_to0] ______________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), shape_to = (1,), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape,shape_to\", RESHAPE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_reshape(shape, shape_to, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[0.24750309]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "shape_to   = (1,)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:210: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8405de8b0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_____________________ test_reshape[cuda-shape1-shape_to1] ______________________\u001b[0m\n",
            "\n",
            "shape = (4, 1, 6), shape_to = (6, 4, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape,shape_to\", RESHAPE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_reshape(shape, shape_to, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 1.7347882 ,  1.9780853 , -2.1093915 ,  0.0146967 ,\n",
            "          0.07289008,  0.33634785]],\n",
            "\n",
            "       [[ 1.129713 ... ]],\n",
            "\n",
            "       [[-0.55685735,  0.17516482, -0.59294105,  0.16579469,\n",
            "         -1.4521552 , -1.1660721 ]]], dtype=float32)\n",
            "device     = cuda()\n",
            "shape      = (4, 1, 6)\n",
            "shape_to   = (6, 4, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:210: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 1, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84050bb20>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 1, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape0] _______________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = (0, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", TRANSPOSE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"axes\", TRANSPOSE_AXES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_transpose(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[0.9637891]]], dtype=float32)\n",
            "axes       = (0, 1)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:221: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8405de6a0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape1] _______________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), axes = (0, 1), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", TRANSPOSE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"axes\", TRANSPOSE_AXES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_transpose(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 0.24229313,  0.3038875 ,  1.0418386 , -0.30553612,\n",
            "          0.4815216 , -0.4495722 ],\n",
            "        [ 1.0149198 ,...036],\n",
            "        [-0.24230671,  0.40026274, -0.5793786 ,  0.18407841,\n",
            "          0.92437047,  0.4315171 ]]], dtype=float32)\n",
            "axes       = (0, 1)\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:221: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8404a5370>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape0] _______________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = (0, 2), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", TRANSPOSE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"axes\", TRANSPOSE_AXES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_transpose(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-1.1008012]]], dtype=float32)\n",
            "axes       = (0, 2)\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:221: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8402c6310>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape1] _______________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), axes = (0, 2), device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", TRANSPOSE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"axes\", TRANSPOSE_AXES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_transpose(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 1.0299685 ,  0.3446224 ,  0.39228645,  0.07826652,\n",
            "         -0.86621696, -1.1352481 ],\n",
            "        [ 1.1580739 ,...68 ],\n",
            "        [ 0.09447403,  0.58481884, -0.23538569, -0.06364735,\n",
            "         -0.39236242, -1.6825415 ]]], dtype=float32)\n",
            "axes       = (0, 2)\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:221: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe84038a310>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape0] _______________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", TRANSPOSE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"axes\", TRANSPOSE_AXES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_transpose(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[1.6210349]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:221: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403713d0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape1] _______________________\u001b[0m\n",
            "\n",
            "shape = (4, 5, 6), axes = None, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape\", TRANSPOSE_SHAPES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"axes\", TRANSPOSE_AXES)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_transpose(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.2345406 , -0.8517744 , -0.4243318 , -1.5320604 ,\n",
            "         -0.00643594,  1.0798386 ],\n",
            "        [ 0.27664822,...47 ],\n",
            "        [ 1.1980885 ,  0.61848694, -1.5373163 , -0.06471775,\n",
            "          1.9226277 ,  2.113707  ]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cuda()\n",
            "shape      = (4, 5, 6)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:221: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (4, 5, 6), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8402c6250>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (4, 5, 6)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m_______________________ test_logsumexp[cuda-shape0-None] _______________________\u001b[0m\n",
            "\n",
            "shape = (1, 1, 1), axes = None, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_logsumexp(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[3.3938375]]], dtype=float32)\n",
            "axes       = None\n",
            "device     = cuda()\n",
            "shape      = (1, 1, 1)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:233: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (1, 1, 1), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8404d6a90>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (1, 1, 1)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape1-0] _________________________\u001b[0m\n",
            "\n",
            "shape = (5, 3), axes = 0, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_logsumexp(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[-1.1693304 , -1.9598371 ,  0.03200281],\n",
            "       [-0.5286519 ,  0.7414541 , -0.63044536],\n",
            "       [-0.8193606 ,  ...83168],\n",
            "       [ 0.92844015, -0.28567657,  0.49430585],\n",
            "       [ 0.8827713 ,  0.69667625, -0.7692884 ]], dtype=float32)\n",
            "axes       = 0\n",
            "device     = cuda()\n",
            "shape      = (5, 3)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:233: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (5, 3), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8402c6d00>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (5, 3)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape2-1] _________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 1, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_logsumexp(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[-0.61970425,  0.49045902],\n",
            "        [-0.74332494, -1.2040609 ],\n",
            "        [-0.543691  , -0.10043301]],\n",
            "\n",
            "       [...  [[ 0.095046  , -0.2642157 ],\n",
            "        [-0.32524756,  0.70726275],\n",
            "        [-0.6719159 , -1.8796829 ]]], dtype=float32)\n",
            "axes       = 1\n",
            "device     = cuda()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:233: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (8, 3, 2), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8403cb2b0>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (8, 3, 2)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape3-2] _________________________\u001b[0m\n",
            "\n",
            "shape = (8, 3, 2), axes = 2, device = cuda()\n",
            "\n",
            "\u001b[1m    @pytest.mark.parametrize(\"shape, axes\", SUMMATION_PARAMETERS)\u001b[0m\n",
            "\u001b[1m    @pytest.mark.parametrize(\"device\", _DEVICES, ids=[\"cpu\", \"cuda\"])\u001b[0m\n",
            "\u001b[1m    def test_logsumexp(shape, axes, device):\u001b[0m\n",
            "\u001b[1m        _A = np.random.randn(*shape).astype(np.float32)\u001b[0m\n",
            "\u001b[1m>       A = ndl.Tensor(nd.array(_A), device=device)\u001b[0m\n",
            "\n",
            "_A         = array([[[ 0.6002251 ,  0.4974558 ],\n",
            "        [-0.5366705 , -0.53319454],\n",
            "        [-0.18542138, -0.64573014]],\n",
            "\n",
            "       [...  [[-0.7940965 ,  1.0068549 ],\n",
            "        [ 1.13256   ,  0.5479147 ],\n",
            "        [-1.2947313 , -0.33982575]]], dtype=float32)\n",
            "axes       = 2\n",
            "device     = cuda()\n",
            "shape      = (8, 3, 2)\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_nd_backend.py\u001b[0m:233: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:220: in __init__\n",
            "\u001b[1m    cached_data = Tensor._array_from_numpy(array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:233: in _array_from_numpy\n",
            "\u001b[1m    return array_api.array(numpy_array, device=device, dtype=dtype)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:665: in array\n",
            "\u001b[1m    return NDArray(a, device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:105: in __init__\n",
            "\u001b[1m    self._init(other.to(device) + 0.0)  # this creates a copy\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:193: in to\n",
            "\u001b[1m    return NDArray(self.numpy(), device=device)\u001b[0m\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:109: in __init__\n",
            "\u001b[1m    array = self.make(other.shape, device=device)\u001b[0m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "shape = (8, 3, 2), strides = None, device = cuda(), handle = None, offset = 0\n",
            "\n",
            "\u001b[1m    @staticmethod\u001b[0m\n",
            "\u001b[1m    def make(shape, strides=None, device=None, handle=None, offset=0):\u001b[0m\n",
            "\u001b[1m        \"\"\"Create a new NDArray with the given properties.  This will allocation the\u001b[0m\n",
            "\u001b[1m            memory if handle=None, otherwise it will use the handle of an existing\u001b[0m\n",
            "\u001b[1m            array.\"\"\"\u001b[0m\n",
            "\u001b[1m        array = NDArray.__new__(NDArray)\u001b[0m\n",
            "\u001b[1m        array._shape = tuple(shape)\u001b[0m\n",
            "\u001b[1m        array._strides = NDArray.compact_strides(shape) if strides is None else strides\u001b[0m\n",
            "\u001b[1m        array._offset = offset\u001b[0m\n",
            "\u001b[1m        array._device = device if device is not None else default_device()\u001b[0m\n",
            "\u001b[1m        if handle is None:\u001b[0m\n",
            "\u001b[1m>           array._handle = array.device.Array(prod(shape))\u001b[0m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: no CUDA-capable device is detected\u001b[0m\n",
            "\n",
            "array      = <[AttributeError(\"'NDArray' object has no attribute '_handle'\") raised in repr()] NDArray object at 0x7fe8402d2c70>\n",
            "device     = cuda()\n",
            "handle     = None\n",
            "offset     = 0\n",
            "shape      = (8, 3, 2)\n",
            "strides    = None\n",
            "\n",
            "\u001b[1m\u001b[31mpython/needle/backend_ndarray/ndarray.py\u001b[0m:145: RuntimeError\n",
            "\u001b[31m\u001b[1m============ 59 failed, 59 passed, 1687 deselected in 16.52 seconds ============\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zw2WRZIDY5R2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}